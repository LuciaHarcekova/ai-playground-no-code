# ğŸ¤– AI & Machine Learning Cheatsheet Summary

*Your comprehensive guide to understanding and applying machine learning concepts*

## ğŸ“‹ Table of Contents

1. [ğŸ¯ What is Machine Learning?](#-what-is-machine-learning)
2. [ğŸ” AI vs Machine Learning](#-ai-vs-machine-learning)
3. [âš™ï¸ Machine Learning Workflow](#ï¸-machine-learning-workflow)
4. [ğŸ“Š Types of Machine Learning](#-types-of-machine-learning)
5. [ğŸ“ Supervised Learning](#-supervised-learning)
   - [Classification Models](#classification-models-categorical-target)
   - [Regression Models](#regression-models-continuous-target)
6. [ğŸ“ˆ Regression vs Classification Summary](#-regression-vs-classification-summary)
7. [ğŸ” Unsupervised Learning](#-unsupervised-learning)
8. [ğŸ® Reinforcement Learning Methods](#-reinforcement-learning-methods)
9. [ğŸ› ï¸ Data Preprocessing](#ï¸-data-preprocessing)
10. [ğŸ§  Deep Learning Fundamentals](#-deep-learning-fundamentals)
11. [âš–ï¸ Bias-Variance Trade-off](#ï¸-bias-variance-trade-off)
12. [âš ï¸ Common Pitfalls](#ï¸-common-pitfalls)
13. [ğŸ“š Additional Resources](#-additional-resources)

---

## ğŸ¯ What is Machine Learning?

**Machine Learning (ML)** is a subset of **Artificial Intelligence (AI)** that enables systems to automatically learn from data and improve performance over time without being explicitly programmed. ML models identify patterns in data to make predictions or decisions.

> **ğŸ’¡ Real-world Example:** Netflix uses ML to analyze your viewing history, preferences, and behavior to recommend movies and shows you're likely to enjoy. The more you watch, the better its recommendations become!

<details>
<summary><strong>ğŸ” Key ML Characteristics (Click to expand)</strong></summary>

- **Data-Driven**: Learns from examples rather than explicit rules
- **Pattern Recognition**: Identifies hidden relationships in data  
- **Predictive Power**: Makes informed predictions on new, unseen data
- **Adaptive**: Improves performance as more data becomes available
- **Automated**: Reduces need for manual programming of rules

</details>

---

## ğŸ” AI vs Machine Learning

* **ğŸ§  Artificial Intelligence (AI):** A broad field aiming to create machines capable of mimicking human intelligence, including reasoning, learning, and problem-solving.
* **ğŸ¤– Machine Learning (ML):** A subset of AI focused on developing algorithms that allow systems to learn patterns from data.

> **ğŸ’¡ Think of it this way:** AI is the destination (intelligent machines), while ML is one of the vehicles to get there (learning from data).

| Aspect | AI | ML |
|--------|----|----|
| **Scope** | Broader field | Subset of AI |
| **Goal** | Mimic human intelligence | Learn from data |
| **Examples** | Chatbots, robots, expert systems | Recommendation engines, fraud detection |
| **Approach** | Multiple techniques | Data-driven algorithms |

---

## âš™ï¸ Machine Learning Workflow

The ML workflow is a systematic approach to building effective machine learning solutions:

```mermaid
graph TD
    A[ğŸ¯ Problem Definition] --> B[ğŸ“Š Data Collection]
    B --> C[ğŸ§¹ Data Preprocessing]
    C --> D[âœ‚ï¸ Data Split]
    D --> E[ğŸ‹ï¸ Model Training]
    E --> F[ğŸ“ Model Evaluation]
    F --> G[ğŸ”§ Hyperparameter Tuning]
    G --> H{Performance OK?}
    H -->|No| E
    H -->|Yes| I[ğŸš€ Model Deployment]
    I --> J[ğŸ“Š Monitor & Update]
    J --> K{Drift Detected?}
    K -->|Yes| B
    K -->|No| J

    style A fill:#ffeb3b
    style I fill:#4caf50
    style J fill:#2196f3
```

### ğŸ“‹ Detailed Workflow Steps:

* **ğŸ¯ Problem Definition:** Define the objective and success criteriaÂ¹
* **ğŸ“Š Data Collection:** Gather relevant datasets from various sources
* **ğŸ§¹ Data Preprocessing:** Clean, encode, and scale featuresÂ²
* **âœ‚ï¸ Data Split:** Divide data into training, validation, and testing sets
* **ğŸ‹ï¸ Model Training:** Fit models on training data using algorithms
* **ğŸ“ Model Evaluation:** Measure performance using appropriate metricsÂ³
* **ğŸ”§ Hyperparameter Tuning:** Optimize model parameters for better performance
* **ğŸš€ Model Deployment:** Integrate the model into production environment
* **ğŸ“Š Monitor & Update:** Track performance and retrain when needed

> **ğŸ’¡ Pro Tip:** The workflow is iterative! Don't expect to get perfect results on the first try. Each iteration teaches you something new about your data and problem.

> **âš ï¸ Common Mistake:** Skipping the problem definition step. Always start with a clear understanding of what you're trying to achieve and how you'll measure success!

---

## ğŸ“Š Types of Machine Learning

Understanding the three main types of ML helps you choose the right approach for your problem:

| Type | ğŸ¯ What it Does | ğŸ“‹ Data Requirement | ğŸ’¼ Example Use Cases | âœ… Benefits | âŒ Limitations |
| ---- | -------------- | ------------------- | ------------------- | ---------- | ------------- |
| **ğŸ“ Supervised Learning** | Learns mapping from input to output using labeled data | Requires labeled data (features + targetâ´) | ğŸ  House price prediction<br/>ğŸ“§ Spam detection<br/>ğŸ©º Medical diagnosis | High accuracy with sufficient labeled data; easy evaluation | Requires large labeled datasets; may overfit |
| **ğŸ” Unsupervised Learning** | Finds hidden patterns without labeled outcomes | Only input features, no target labels | ğŸ‘¥ Customer segmentation<br/>ğŸš¨ Anomaly detection<br/>ğŸ“° Topic modeling | Explores unknown data; reveals hidden structures | Hard to interpret; no accuracy guarantee |
| **ğŸ® Reinforcement Learning** | Learns through environment interaction and rewards/penalties | Requires environment with feedback system | ğŸš— Self-driving cars<br/>ğŸ¯ Game-playing AI<br/>ğŸ¤– Robotics control | Learns complex sequential tasks; adapts through experience | Computationally expensive; slow convergence |

> **ğŸ’¡ Quick Decision Guide:**
> - Have labeled data? â†’ **Supervised Learning**
> - Want to find patterns in unlabeled data? â†’ **Unsupervised Learning**  
> - Need to learn through trial and error? â†’ **Reinforcement Learning**

---

## ğŸ“ Supervised Learning

**Supervised learning** trains models on labeled data to learn the mapping from input (features) to output (labels/values). Think of it as learning with a teacher who provides the "correct answers."

> **ğŸ’¡ Real-world Example:** Teaching a child to recognize animals by showing them pictures labeled "cat," "dog," "bird." After seeing many examples, they can identify animals in new photos.

### ğŸ·ï¸ Classification Models (Categorical targetâµ)

*When you need to predict **categories** or **classes***

| Model               | Explanation                                   | Example Usage                    |
| ------------------- | --------------------------------------------- | -------------------------------- |
| Logistic Regression | Models probability of binary outcomes         | Email spam detection             |
| Decision Trees      | Classifies using feature-based rules          | Loan approval                    |
| Random Forests      | Ensemble of decision trees                    | Diagnosing diseases              |
| SVM                 | Finds best separating hyperplane              | Handwritten digit classification |
| k-NN                | Assigns label based on nearest neighbors      | Product recommendation           |
| NaÃ¯ve Bayes         | Probabilistic classifier using Bayesâ€™ theorem | Sentiment analysis               |
| Gradient Boosting   | Sequential boosted trees for high accuracy    | Fraud detection                  |
| Neural Networks     | Layers of neurons for complex patterns        | Face recognition                 |

<details>
<summary><strong>ğŸ“Š Classification Evaluation Metrics (Click to expand)</strong></summary>

* **ğŸ¯ Accuracy:** Correct predictions / Total â†’ Fraction correct; **misleading for imbalanced data**
* **ğŸ” Precision:** TPâ¶ / (TP+FPâ·) â†’ Of predicted positives, how many are truly positive
* **ğŸ“ˆ Recall (Sensitivity):** TP / (TP+FNâ¸) â†’ Of actual positives, how many were identified  
* **âš–ï¸ F1-Score:** Harmonic mean of precision & recall â†’ **Balances both metrics**
* **ğŸ“Š ROC-AUC:** Area under ROC curve â†’ **AUC closer to 1 = better discrimination**
* **ğŸ”³ Confusion Matrix:** Visual table of TP, TNâ¹, FP, FN â†’ **See exactly where errors occur**

**ğŸ’¡ Metric Selection Guide:**
- **Accuracy**: When classes are balanced
- **Precision**: When false positives are costly (e.g., spam detection)
- **Recall**: When false negatives are costly (e.g., disease detection)
- **F1-Score**: When you need balance between precision and recall

</details>

### ğŸ“Š Regression Models (Continuous targetÂ¹â°)

*When you need to predict **numerical values***

> **ğŸ’¡ Real-world Example:** Predicting house prices based on features like size, location, age, and amenities. Unlike classification which gives categories, regression provides exact numerical predictions.

| Model | ğŸ” Explanation | ğŸ¯ Example Usage | ğŸ’¡ When to Use |
| ----- | ------------- | --------------- | -------------- |
| **Linear Regression** | Fits straight line relationship between features and target | ğŸ  House price prediction | Simple linear relationships; interpretable |
| **Polynomial Regression** | Captures curved relationships with polynomial features | ğŸ“ˆ Population growth modeling | Non-linear but smooth patterns |
| **Ridge & Lasso Regression** | Regularized linear models preventing overfitting | ğŸ“Š Stock price prediction | Many features; need feature selection |
| **Decision Tree Regression** | Creates rules based on feature value splits | ğŸš— Car value prediction | Non-linear patterns; interpretable rules |
| **Random Forest Regression** | Ensemble of trees for robust predictions | ğŸ’° Sales revenue forecasting | High accuracy; handles overfitting |
| **Support Vector Regression (SVR)** | Finds best-fit boundary with error tolerance | ğŸŒ¡ï¸ Temperature prediction | Complex patterns; robust to outliers |
| **Gradient Boosting** | Sequential models correcting previous errors | âš¡ Electricity demand forecasting | Maximum accuracy needed |

<details>
<summary><strong>ğŸ“ Regression Evaluation Metrics (Click to expand)</strong></summary>

* **ğŸ“ MAEÂ¹Â¹:** Average absolute differences â†’ **How far predictions are from actual values**
* **ğŸ¯ MSEÂ¹Â²:** Average squared differences â†’ **Penalizes large errors; sensitive to outliers**
* **ğŸ“ RMSEÂ¹Â³:** Square root of MSE â†’ **Same unit as target; easily interpretable**
* **ğŸ–ï¸ RÂ² Score:** Proportion of variance explained â†’ **0 = no fit, 1 = perfect fit**
* **ğŸ”§ Adjusted RÂ²:** RÂ² adjusted for number of predictors â†’ **Useful for multiple regression**

**ğŸ’¡ Metric Selection Guide:**
- **MAE**: When all errors are equally important
- **RMSE**: When large errors are more problematic than small ones
- **RÂ²**: To understand model's explanatory power
- **Adjusted RÂ²**: When comparing models with different numbers of features

</details>

---

## ğŸ“ˆ Regression vs Classification Summary

Quick reference to help you choose between regression and classification:

| Aspect | ğŸ“Š Regression | ğŸ·ï¸ Classification |
| ------ | ------------ | ---------------- |
| **ğŸ¯ Target Variable** | Continuous numbers | Discrete categories |
| **ğŸ¨ Goal** | Predict exact numeric values | Assign to predefined categories |
| **ğŸ’¼ Example Problems** | ğŸ  House prices, ğŸ“ˆ Stock prices, ğŸŒ¡ï¸ Temperature | ğŸ“§ Spam detection, ğŸ©º Disease diagnosis, ğŸ‘¤ Image recognition |
| **âš™ï¸ Algorithms** | Linear, Polynomial, Ridge/Lasso, Trees, RF, SVR, Boosting | Logistic, Trees, Random Forests, SVM, k-NN, NaÃ¯ve Bayes, Neural Networks |
| **ğŸ“ Evaluation Metrics** | MAE, MSE, RMSE, RÂ² | Accuracy, Precision, Recall, F1, ROC-AUC |
| **ğŸ Output** | Real number (e.g., $250,000) | Class label (e.g., "Spam") or probability |
| **ğŸ¢ Use Cases** | Finance, real estate, forecasting | Healthcare, fraud detection, image/speech recognition |
| **âš ï¸ Main Challenges** | Sensitive to outliers; assumes numeric relationships | Class imbalance, overfitting, interpretability |

> **ğŸ’¡ Quick Decision:** Ask yourself "What am I predicting?" If it's a number â†’ Regression. If it's a category â†’ Classification.

---

## ğŸ” Unsupervised Learning

**Unsupervised learning** finds hidden patterns in data without labeled examples. Think of it as exploring data to discover insights you didn't know existed.

> **ğŸ’¡ Real-world Example:** A streaming service analyzing viewing patterns to discover that people who watch sci-fi also tend to watch documentaries, even though no one told it to look for this connection.

### ğŸ¯ Clustering
*Finding groups of similar data points*

<details>
<summary><strong>ğŸ” Clustering Algorithms (Click to expand)</strong></summary>

* **ğŸ¯ k-Means:** Groups data into k clusters using centroids â†’ ğŸ‘¥ Customer segmentation for marketing
* **ğŸŒ³ Hierarchical Clustering:** Creates nested clusters in tree structure â†’ ğŸ“„ Document grouping by topic
* **ğŸ” DBSCAN:** Finds clusters based on data density â†’ ğŸš¨ Anomaly detection in network traffic
* **ğŸ² GMMÂ¹â´:** Probabilistic soft assignment to clusters â†’ ğŸ–¼ï¸ Image segmentation
* **ğŸ“ Mean Shift:** Automatically discovers number of clusters â†’ ğŸ—ºï¸ Geographical hotspot detection

**ğŸ’¡ Clustering Tips:**
- Start with k-Means for simplicity
- Use DBSCAN when you don't know the number of clusters
- Try Hierarchical for interpretable cluster relationships

</details>

### ğŸ“ Dimensionality Reduction
*Simplifying data while keeping important information*

<details>
<summary><strong>ğŸ“ Dimensionality Reduction Techniques (Click to expand)</strong></summary>

* **ğŸ“Š PCAÂ¹âµ:** Reduces dimensions while retaining maximum variance â†’ ğŸ–¼ï¸ Image compression and noise reduction
* **ğŸ”¢ SVDÂ¹â¶:** Factorizes matrices into simpler components â†’ ğŸ¬ Recommendation systems (Netflix, Amazon)
* **ğŸ¨ t-SNEÂ¹â·:** Visualizes high-dimensional data in 2D/3D â†’ ğŸ“ Word embeddings visualization
* **ğŸ§  Autoencoders:** Neural networks that compress and reconstruct â†’ ğŸ”Š Noise reduction, ğŸ–¼ï¸ image denoising

**ğŸ’¡ When to Use:**
- **PCA**: When you need linear dimensionality reduction and interpretability
- **t-SNE**: For visualization and exploratory data analysis
- **Autoencoders**: For non-linear reduction and when working with images/complex data

</details>

---

## ğŸ® Reinforcement Learning Methods

**Reinforcement Learning (RL)** teaches agents to make decisions through trial and error, learning from rewards and penalties.

> **ğŸ’¡ Real-world Example:** Teaching a robot to walk by rewarding it for forward movement and penalizing it for falling. Over time, it learns optimal walking strategies.

<details>
<summary><strong>ğŸ® RL Algorithms & Applications (Click to expand)</strong></summary>

* **ğŸ—ºï¸ Q-Learning:** Learns optimal actions using Q-table â†’ ğŸ¤– Robot maze navigation
* **ğŸ§  Deep Q-Networks (DQN):** Combines Q-learning with deep learning â†’ ğŸ® Playing Atari games
* **ğŸš¶ SARSAÂ¹â¸:** Learns from actual actions taken â†’ ğŸš— Safe autonomous driving policies
* **ğŸ¯ Policy Gradient:** Directly optimizes decision policies â†’ ğŸ¦¾ Robotic arm control
* **ğŸ­ Actor-Critic:** Combines value and policy learning â†’ ğŸ•¹ï¸ Continuous control tasks
* **ğŸ›¡ï¸ PPO & TRPO:** Stable policy optimization â†’ ğŸ¦¾ Humanoid robot locomotion
* **ğŸ² Monte Carlo:** Learns from complete episode experiences â†’ â™Ÿï¸ Game strategy simulations

**ğŸ’¡ RL Key Concepts:**
- **Agent**: The decision maker (robot, game player)
- **Environment**: The world the agent interacts with
- **Reward**: Feedback signal for actions (+1 for good, -1 for bad)
- **Policy**: Strategy for choosing actions

</details>

---

## ğŸ› ï¸ Data Preprocessing

**Data preprocessing** is crucial for ML success - garbage in, garbage out! Clean, well-prepared data often matters more than the algorithm choice.

> **ğŸ’¡ Real-world Insight:** Data scientists spend 80% of their time on data cleaning and preprocessing, only 20% on actual modeling!

<details>
<summary><strong>ğŸ§¹ Essential Preprocessing Steps (Click to expand)</strong></summary>

### ğŸš« Handle Missing Values
* **Remove**: Drop rows/columns with missing data â†’ When data is abundant
* **Fill**: Replace with mean/median/mode â†’ For numerical features
* **Interpolate**: Estimate based on neighboring values â†’ For time series data

### ğŸ·ï¸ Encode Categorical Data
* **ğŸ”¢ LabelEncoder**: Convert categories to numbers (0,1,2...) â†’ For ordinal data
* **ğŸ¯ OneHotEncoder**: Create binary columns for each category â†’ For nominal data
* **ğŸ“Š OrdinalEncoder**: Preserve order in categories â†’ For ranked data (small, medium, large)

### âš–ï¸ Feature Scaling
* **ğŸ“ StandardScaler**: Mean=0, std=1 (Z-score normalization) â†’ For algorithms sensitive to scale
* **ğŸ“ MinMaxScaler**: Scale to range [0,1] â†’ When you need bounded values
* **ğŸ›¡ï¸ RobustScaler**: Uses median and IQR â†’ When data has outliers

**âš ï¸ Preprocessing Gotchas:**
- **Never** fit scalers on test data - leads to data leakage!
- Handle missing values **before** splitting data
- Be careful with time series - don't use future data to predict past!

</details>

---

## ğŸ§  Deep Learning Fundamentals

**Deep Learning** uses neural networks with multiple layers to learn complex patterns. Think of it as inspired by how the human brain processes information.

> **ğŸ’¡ Real-world Example:** Your smartphone camera recognizing faces uses Convolutional Neural Networks (CNNs) to understand visual patterns like edges, shapes, and eventually complete faces.

<details>
<summary><strong>ğŸ§  Neural Network Types & Applications (Click to expand)</strong></summary>

* **ğŸŒ ANNÂ¹â¹ (Artificial Neural Networks):** General-purpose networks â†’ ğŸ  Housing price prediction, ğŸ“Š tabular data analysis
* **ğŸ‘ï¸ CNNÂ²â° (Convolutional Networks):** Specialized for images with convolution layers â†’ ğŸ“· Object detection, ğŸ©º medical imaging, ğŸš— self-driving cars
* **ğŸ”„ RNNÂ²Â¹ (Recurrent Networks):** Handle sequential data with memory â†’ ğŸ“ Next word prediction, ğŸµ music generation, ğŸ“ˆ time series forecasting
* **ğŸ¤– Transformers (BERT, GPT):** Attention-based architecture â†’ ğŸˆº Machine translation, ğŸ’¬ ChatGPT, ğŸ“° text summarization

**ğŸ’¡ Deep Learning Guidelines:**
- **CNN**: Use for images, spatial data
- **RNN/LSTM**: Use for sequences, time series
- **Transformers**: Use for language, complex sequences
- **ANN**: Use for structured/tabular data

**âš ï¸ Deep Learning Requirements:**
- Large datasets (thousands to millions of examples)
- Significant computational resources (GPUs recommended)
- Longer training times compared to traditional ML

</details>

---

## âš–ï¸ Bias-Variance Trade-off

Understanding this fundamental concept helps you build better models and avoid common ML pitfalls.

> **ğŸ’¡ Think of it like archery:** Bias is consistently missing the target in the same direction (systematic error). Variance is shots scattered around (random error). Good models have both low bias AND low variance.

<details>
<summary><strong>âš–ï¸ Bias-Variance Breakdown (Click to expand)</strong></summary>

* **ğŸ¯ Bias:** Error from overly simple models that miss important patterns (**underfitting**)
  - *Example:* Using linear regression for clearly non-linear data
  - *Solutions:* Use more complex models, add polynomial features

* **ğŸ“Š Variance:** Error from overly complex models that learn noise (**overfitting**)
  - *Example:* Decision tree that memorizes training data perfectly
  - *Solutions:* Use regularization, get more data, ensemble methods

* **ğŸ–ï¸ Goal:** Find the sweet spot that **balances both for optimal generalization**

**ğŸ’¡ Practical Tips:**
- **High Bias**: Try more complex models, add features
- **High Variance**: Add regularization, get more data, use ensemble methods
- **Both**: Start simple, then gradually increase complexity

</details>

---

## âš ï¸ Common Pitfalls

Avoid these expensive mistakes that even experienced practitioners make!

> **ğŸ’¡ Prevention Insight:** Most ML failures come from data issues, not algorithm choice. Focus on data quality first!

### ğŸš¨ Critical Mistakes to Avoid:

<details>
<summary><strong>âš ï¸ Data Leakage (Click to expand)</strong></summary>

**What it is:** When information from the future or target variable accidentally influences training.

**Examples:**
- Using tomorrow's stock price to predict today's price
- Including the target variable as a feature (directly or indirectly)
- Scaling using statistics from the entire dataset before splitting

**ğŸ›¡ï¸ Prevention:**
- Always split data BEFORE any preprocessing
- Be suspicious of "too good to be true" results
- Carefully examine feature correlations with target

</details>

<details>
<summary><strong>âš–ï¸ Imbalanced Data (Click to expand)</strong></summary>

**What it is:** When some classes have much fewer examples than others.

**Example:** Fraud detection where 99.9% of transactions are legitimate.

**ğŸ›¡ï¸ Solutions:**
- Use appropriate metrics (Precision, Recall, F1 instead of just Accuracy)
- Apply sampling techniques (SMOTE, undersampling, oversampling)
- Use class weights in algorithms
- Consider ensemble methods

</details>

<details>
<summary><strong>ğŸ¯ Overfitting / Underfitting (Click to expand)</strong></summary>

**Overfitting:** Model learns training data too well, fails on new data.
- *Signs:* Perfect training accuracy, poor test performance
- *Solutions:* More data, regularization, simpler models, cross-validation

**Underfitting:** Model too simple to capture underlying patterns.
- *Signs:* Poor performance on both training and test data
- *Solutions:* More complex models, additional features, reduce regularization

**ğŸ’¡ Golden Rule:** Always validate on data the model has never seen!

</details>

---

## ğŸ“š Additional Resources

### ğŸ“ Learning Platforms
- **[Coursera ML Course](https://www.coursera.org/learn/machine-learning)** - Andrew Ng's famous introduction to ML
- **[Fast.ai](https://www.fast.ai/)** - Practical deep learning for coders
- **[Kaggle Learn](https://www.kaggle.com/learn)** - Free micro-courses with hands-on practice

### ğŸ“– Essential Books
- **[Hands-On ML](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)** - Practical guide with Scikit-learn and TensorFlow
- **[Pattern Recognition and ML](https://www.microsoft.com/en-us/research/publication/pattern-recognition-machine-learning/)** - Mathematical foundations

### ğŸ› ï¸ Tools & Libraries
- **[Scikit-learn](https://scikit-learn.org/)** - Python's go-to ML library
- **[TensorFlow](https://www.tensorflow.org/)/[PyTorch](https://pytorch.org/)** - Deep learning frameworks
- **[Jupyter Notebooks](https://jupyter.org/)** - Interactive development environment

### ğŸ† Practice Platforms
- **[Kaggle](https://www.kaggle.com/)** - Competitions and datasets
- **[Google Colab](https://colab.research.google.com/)** - Free GPU/TPU for experiments

---

## ğŸ“ Footnotes

Â¹ **Problem Definition**: Clearly state what you want to predict and how you'll measure success  
Â² **Data Preprocessing**: Clean and prepare raw data for ML algorithms  
Â³ **Performance Metrics**: Quantitative measures to evaluate model quality  
â´ **Target Variable**: The outcome you want to predict (also called label or dependent variable)  
âµ **Categorical Target**: Discrete categories like "spam/not spam" or "cat/dog/bird"  
â¶ **TP (True Positive)**: Correctly predicted positive cases  
â· **FP (False Positive)**: Incorrectly predicted positive cases (Type I error)  
â¸ **FN (False Negative)**: Incorrectly predicted negative cases (Type II error)  
â¹ **TN (True Negative)**: Correctly predicted negative cases  
Â¹â° **Continuous Target**: Numerical values like prices, temperatures, or distances  
Â¹Â¹ **MAE (Mean Absolute Error)**: Average of absolute differences between predicted and actual values  
Â¹Â² **MSE (Mean Squared Error)**: Average of squared differences between predicted and actual values  
Â¹Â³ **RMSE (Root Mean Square Error)**: Square root of MSE, same units as target variable  
Â¹â´ **GMM (Gaussian Mixture Model)**: Probabilistic clustering using Gaussian distributions  
Â¹âµ **PCA (Principal Component Analysis)**: Linear dimensionality reduction technique  
Â¹â¶ **SVD (Singular Value Decomposition)**: Matrix factorization technique  
Â¹â· **t-SNE (t-Distributed Stochastic Neighbor Embedding)**: Non-linear dimensionality reduction for visualization  
Â¹â¸ **SARSA (State-Action-Reward-State-Action)**: On-policy reinforcement learning algorithm  
Â¹â¹ **ANN (Artificial Neural Network)**: Basic neural network with fully connected layers  
Â²â° **CNN (Convolutional Neural Network)**: Neural network designed for processing grid-like data (images)  
Â²Â¹ **RNN (Recurrent Neural Network)**: Neural network designed for sequential data with memory
